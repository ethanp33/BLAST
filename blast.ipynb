{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blast Application - By Ethan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from Bio.Blast import NCBIWWW, NCBIXML\n",
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "from lxml import etree as et\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the previously made command line script, we can write a function that inputs a file name (including the file extension) and run a blast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blast(file_name: str, blast_type: str, max_hits: int = None, megablast: bool = False, e_value_threshold: float = None):\n",
    "    \"\"\"\n",
    "    A function that runs a BLAST of desired type and saves the results as an xml file. \n",
    "    :inputs: file_name (including file extension), blast_type (one of blastn, blastp, tblastn or tblastx)\n",
    "    :returns: None\n",
    "    \"\"\"\n",
    "\n",
    "    # Error check\n",
    "    if blast_type.lower() in [\"blastn\", \"blastp\", \"blastx\", \"tblastn\", \"tblastx\"]:\n",
    "        \n",
    "        # Open sequence file\n",
    "        seq_file = next(SeqIO.parse(open(file_name), \"fasta\"))\n",
    "\n",
    "        # Megablast\n",
    "        if blast_type == \"blastn\":\n",
    "            if megablast == True:\n",
    "                query = NCBIWWW.qblast(blast_type, \"nt\", seq_file.seq, megablast=True)\n",
    "\n",
    "        # Start query\n",
    "        print(\"Starting BLAST query on NCBI database...\")\n",
    "        print(\"Please hold. This process can take up to 10 minutes to complete...\")\n",
    "        query = NCBIWWW.qblast(blast_type, \"nt\", seq_file.seq)\n",
    "        print(query)\n",
    "\n",
    "        # Store results as XML\n",
    "        print(\"Storing results as an XML file...\")\n",
    "        with open(str(file_name).split(\".\")[0] + \"_results.xml\", \"w\") as save_file:\n",
    "            blast_results = query.read()\n",
    "            save_file.write(blast_results)\n",
    "\n",
    "        print(\"Success! \" + str(file_name).split(\".\")[0] + \"_results.xml has been saved!\")\n",
    "        \n",
    "        # Make raw dataframe\n",
    "        print(\"Parsing XML file as dataframe...\")\n",
    "        df = pd.read_xml(str(file_name).split(\".\")[0] + \"_results.xml\", xpath=\".//Hsp\")\n",
    "        \n",
    "        # Edit raw dataframe\n",
    "        # Rename and drop columns\n",
    "        df = df.drop([\"Hsp_num\", \"Hsp_qseq\", \"Hsp_query-frame\", \"Hsp_hit-frame\"], axis=1)\n",
    "        df = df.rename(columns={\"Hsp_bit-score\" : \"bit_score\", \"Hsp_score\" : \"score\", \"Hsp_evalue\" : \"evalue\", \"Hsp_query-from\" : \"query_from\", \"Hsp_query-to\" : \"query_to\",\n",
    "        \"Hsp_hit-from\" : \"hit_from\", \"Hsp_hit-to\" : \"hit_to\", \"Hsp_identity\" : \"identity\", \"Hsp_positive\" : \"positive\", \"Hsp_gaps\" : \"gaps\", \"Hsp_align-len\" : \"align_len\",\n",
    "        \"Hsp_hseq\" : \"hit_seq\", \"Hsp_midline\" : \"midline\"})\n",
    "\n",
    "        # Add new columns based on Hit XML alltributes\n",
    "        tree = et.parse(open(str(file_name).split(\".\")[0] + \"_results.xml\"))\n",
    "        hit_id = tree.xpath(\".//Hit/Hit_id/text()\")\n",
    "        hit_def = tree.xpath(\".//Hit/Hit_def/text()\")\n",
    "        hit_accession = tree.xpath(\".//Hit/Hit_accession/text()\")\n",
    "        hit_length = tree.xpath(\".//Hit/Hit_len/text()\")\n",
    "        df[\"id\"] = hit_id\n",
    "        df[\"description\"] = hit_def\n",
    "        df[\"accession\"] = hit_accession\n",
    "        df[\"acc_len\"] = hit_length\n",
    "\n",
    "        # Calculating percent identity\n",
    "        df[\"per_ident\"] = np.round((df[\"identity\"] / df[\"align_len\"])*100, decimals=2)\n",
    "\n",
    "        # Calculating query coverage\n",
    "        df[\"query_cover\"] = np.round((df[\"query_to\"] - df[\"query_from\"] + 1)/len(seq_file.seq)*100, decimals=2)\n",
    "\n",
    "        # Change order of columns\n",
    "        df = df[[\"id\", \"description\", \"bit_score\", \"score\", \"query_cover\", \"per_ident\", \"evalue\", \"query_from\", \"query_to\", \n",
    "        \"hit_from\", \"hit_to\", \"identity\", \"positive\", \"gaps\", \"align_len\", \"acc_len\", \"hit_seq\", \"midline\", \"accession\"]]\n",
    "\n",
    "        # Remove rows if e-value is over threshold\n",
    "        if e_value_threshold is not None:\n",
    "            df = df[df.evalue < e_value_threshold]\n",
    "        \n",
    "        # Cap rows/hits if there is a maximum\n",
    "        if max_hits is not None:\n",
    "            df = df.sort_values(by=\"evalue\", ascending=False)\n",
    "            df = df.head(max_hits)\n",
    "\n",
    "        # Save editted dataframe\n",
    "        print(\"Saving dataframe as csv file...\")\n",
    "        df.to_csv(str(file_name).split(\".\")[0] + \"_dataframe.csv\")\n",
    "\n",
    "        # End\n",
    "        print(\"Success! Dataframe has successfully been saved as \" + str(file_name).split(\".\")[0] + \"_dataframe.csv\" + \". Press any key to exit function.\")\n",
    "    else:\n",
    "        print(\"blast_type not one of blastn, blastp, blastx, tblastn or tblastx\")\n",
    "        blast(file_name=file_name, blast_type=blast_type, max_hits=max_hits, megablast=megablast, e_value_threshold=e_value_threshold)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a test of the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BLAST query on NCBI database...\n",
      "Please hold. This process can take up to 10 minutes to complete...\n",
      "<_io.StringIO object at 0x000001D9880EEC10>\n",
      "Storing results as an XML file...\n",
      "Success! blast\\AB252222_results.xml has been saved!\n",
      "Parsing XML file as dataframe...\n",
      "Saving dataframe as csv file...\n",
      "Success! Dataframe has successfully been saved as blast\\AB252222_dataframe.csv. Press any key to exit function.\n"
     ]
    }
   ],
   "source": [
    "blast(file_name=\"blast\\\\AB252222.1.fna\", blast_type=\"blastn\", max_hits=5, megablast=False, e_value_threshold=1e-20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing the dataframe. Here is the csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>bit_score</th>\n",
       "      <th>score</th>\n",
       "      <th>query_cover</th>\n",
       "      <th>per_ident</th>\n",
       "      <th>evalue</th>\n",
       "      <th>query_from</th>\n",
       "      <th>query_to</th>\n",
       "      <th>hit_from</th>\n",
       "      <th>hit_to</th>\n",
       "      <th>identity</th>\n",
       "      <th>positive</th>\n",
       "      <th>gaps</th>\n",
       "      <th>align_len</th>\n",
       "      <th>acc_len</th>\n",
       "      <th>hit_seq</th>\n",
       "      <th>midline</th>\n",
       "      <th>accession</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>gi|1829765830|ref|NC_046733.1|</td>\n",
       "      <td>Aporrectodea rosea haplogroup L4 mitochondrion...</td>\n",
       "      <td>2778.47</td>\n",
       "      <td>3080</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1540</td>\n",
       "      <td>1</td>\n",
       "      <td>1540</td>\n",
       "      <td>1540</td>\n",
       "      <td>1540</td>\n",
       "      <td>0</td>\n",
       "      <td>1540</td>\n",
       "      <td>15086</td>\n",
       "      <td>ATGCGATGATTTTACTCAACCAATCACAAAGATATTGGAACTTTAT...</td>\n",
       "      <td>||||||||||||||||||||||||||||||||||||||||||||||...</td>\n",
       "      <td>NC_046733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>gi|1068288817|gb|KU728851.1|</td>\n",
       "      <td>Enchytraeus cf. crypticus SL-2017 cytochrome c...</td>\n",
       "      <td>1349.30</td>\n",
       "      <td>1495</td>\n",
       "      <td>99.09</td>\n",
       "      <td>79.62</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1526</td>\n",
       "      <td>1</td>\n",
       "      <td>1523</td>\n",
       "      <td>1215</td>\n",
       "      <td>1215</td>\n",
       "      <td>3</td>\n",
       "      <td>1526</td>\n",
       "      <td>1539</td>\n",
       "      <td>ATGCGCTGACTATATTCTACAAACCACAAAGACATTGGTACACTAT...</td>\n",
       "      <td>||||| ||| | || || || || |||||||| ||||| ||  |||...</td>\n",
       "      <td>KU728851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "      <td>gi|1003312217|gb|KT429019.1|</td>\n",
       "      <td>Amynthas robustus mitochondrion, complete genome</td>\n",
       "      <td>1387.18</td>\n",
       "      <td>1537</td>\n",
       "      <td>99.74</td>\n",
       "      <td>80.01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1536</td>\n",
       "      <td>1</td>\n",
       "      <td>1536</td>\n",
       "      <td>1229</td>\n",
       "      <td>1229</td>\n",
       "      <td>0</td>\n",
       "      <td>1536</td>\n",
       "      <td>15013</td>\n",
       "      <td>ATGCGATGATTATATTCTACAAACCACAAAGACATTGGAACCCTAT...</td>\n",
       "      <td>||||||||||| || || || || |||||||| ||||||||  |||...</td>\n",
       "      <td>KT429019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>gi|827022294|gb|KP688582.1|</td>\n",
       "      <td>Amynthas gracilis mitochondrion, complete genome</td>\n",
       "      <td>1381.77</td>\n",
       "      <td>1531</td>\n",
       "      <td>99.22</td>\n",
       "      <td>80.04</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1528</td>\n",
       "      <td>1</td>\n",
       "      <td>1528</td>\n",
       "      <td>1223</td>\n",
       "      <td>1223</td>\n",
       "      <td>0</td>\n",
       "      <td>1528</td>\n",
       "      <td>15161</td>\n",
       "      <td>ATGCGATGATTGTATTCTACAAACCACAAAGACATTGGAACCCTAT...</td>\n",
       "      <td>||||||||||| || || || || |||||||| ||||||||  |||...</td>\n",
       "      <td>KP688582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>gi|1003312161|gb|KT429015.1|</td>\n",
       "      <td>Duplodicodrilus schmardae mitochondrion, compl...</td>\n",
       "      <td>1379.96</td>\n",
       "      <td>1529</td>\n",
       "      <td>99.81</td>\n",
       "      <td>79.90</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1537</td>\n",
       "      <td>1</td>\n",
       "      <td>1537</td>\n",
       "      <td>1228</td>\n",
       "      <td>1228</td>\n",
       "      <td>0</td>\n",
       "      <td>1537</td>\n",
       "      <td>15156</td>\n",
       "      <td>ATGCGATGATTATATTCTACAAATCACAAAGACATTGGAACTTTAT...</td>\n",
       "      <td>||||||||||| || || || ||||||||||| |||||||||||||...</td>\n",
       "      <td>KT429015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                              id  \\\n",
       "0           0  gi|1829765830|ref|NC_046733.1|   \n",
       "1          37    gi|1068288817|gb|KU728851.1|   \n",
       "2          27    gi|1003312217|gb|KT429019.1|   \n",
       "3          28     gi|827022294|gb|KP688582.1|   \n",
       "4          29    gi|1003312161|gb|KT429015.1|   \n",
       "\n",
       "                                         description  bit_score  score  \\\n",
       "0  Aporrectodea rosea haplogroup L4 mitochondrion...    2778.47   3080   \n",
       "1  Enchytraeus cf. crypticus SL-2017 cytochrome c...    1349.30   1495   \n",
       "2   Amynthas robustus mitochondrion, complete genome    1387.18   1537   \n",
       "3   Amynthas gracilis mitochondrion, complete genome    1381.77   1531   \n",
       "4  Duplodicodrilus schmardae mitochondrion, compl...    1379.96   1529   \n",
       "\n",
       "   query_cover  per_ident  evalue  query_from  query_to  hit_from  hit_to  \\\n",
       "0       100.00     100.00       0           1      1540         1    1540   \n",
       "1        99.09      79.62       0           1      1526         1    1523   \n",
       "2        99.74      80.01       0           1      1536         1    1536   \n",
       "3        99.22      80.04       0           1      1528         1    1528   \n",
       "4        99.81      79.90       0           1      1537         1    1537   \n",
       "\n",
       "   identity  positive  gaps  align_len  acc_len  \\\n",
       "0      1540      1540     0       1540    15086   \n",
       "1      1215      1215     3       1526     1539   \n",
       "2      1229      1229     0       1536    15013   \n",
       "3      1223      1223     0       1528    15161   \n",
       "4      1228      1228     0       1537    15156   \n",
       "\n",
       "                                             hit_seq  \\\n",
       "0  ATGCGATGATTTTACTCAACCAATCACAAAGATATTGGAACTTTAT...   \n",
       "1  ATGCGCTGACTATATTCTACAAACCACAAAGACATTGGTACACTAT...   \n",
       "2  ATGCGATGATTATATTCTACAAACCACAAAGACATTGGAACCCTAT...   \n",
       "3  ATGCGATGATTGTATTCTACAAACCACAAAGACATTGGAACCCTAT...   \n",
       "4  ATGCGATGATTATATTCTACAAATCACAAAGACATTGGAACTTTAT...   \n",
       "\n",
       "                                             midline  accession  \n",
       "0  ||||||||||||||||||||||||||||||||||||||||||||||...  NC_046733  \n",
       "1  ||||| ||| | || || || || |||||||| ||||| ||  |||...   KU728851  \n",
       "2  ||||||||||| || || || || |||||||| ||||||||  |||...   KT429019  \n",
       "3  ||||||||||| || || || || |||||||| ||||||||  |||...   KP688582  \n",
       "4  ||||||||||| || || || ||||||||||| |||||||||||||...   KT429015  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"aporrectodea_rosea_dataframe.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing BLAST multiple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blast_multiple(folder_directory: str, blast_type: str, max_hits: int = None, megablast: bool = False, e_value_threshold: float = None):\n",
    "    \"\"\"\n",
    "    A function that runs multiple BLASTs of a desired type and saves the results as a csv file.\n",
    "    :inputs: folder_directory, blast_type (one of blastn, blastp, tblastn or tblastx), \n",
    "            max_hits, megablast, e_value_threshold\n",
    "    :returns: None\n",
    "    \"\"\" \n",
    "\n",
    "    # Imports\n",
    "    from Bio.Blast import NCBIWWW, NCBIXML\n",
    "    from Bio import SeqIO\n",
    "    import pandas as pd\n",
    "    from lxml import etree as et\n",
    "    import numpy as np\n",
    "    from blast import blast\n",
    "\n",
    "    # Get each file\n",
    "    file_names = []\n",
    "    count = 0\n",
    "    for filename in os.listdir(folder_directory):\n",
    "        f = os.path.join(folder_directory, filename)\n",
    "        # Check if the file exists\n",
    "        if os.path.isfile(f) and filename.split(\".\")[-1] == \"fna\":\n",
    "            count += 1\n",
    "            print(\"Searching \" + folder_directory + \". Found \" + str(count) + \" sequences to BLAST so far.\", end=\"\\r\")\n",
    "            file_names.append(str(f))\n",
    "\n",
    "    for file_name in file_names:\n",
    "        count = 0\n",
    "        print(\"Currenting BLASTing \" + str(file_name) + \". Have BLASTed \" + str(count) + \" sequences so far.\", end=\"\\r\")\n",
    "        blast(file_name=file_name, blast_type=blast_type, max_hits=max_hits, megablast=megablast, e_value_threshold=e_value_threshold)\n",
    "    print(\"Success! BlASTed a total of \" + str(count) + \" sequences.\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BLAST query on NCBI database...riensis.fna. Have BLASTed 0 sequences so far.\n",
      "Please hold. This process can take up to 10 minutes to complete...\n",
      "<_io.StringIO object at 0x00000174906950D0>\n",
      "Storing results as an XML file...\n",
      "Success! to_blast\\amynthas_jiriensis_results.xml has been saved!\n",
      "Parsing XML file as dataframe...\n",
      "Saving dataframe as csv file...\n",
      "Success! Dataframe has successfully been saved as to_blast\\amynthas_jiriensis_dataframe.csv. Press any key to exit function.\n",
      "Starting BLAST query on NCBI database...ungpanensis.fna. Have BLASTed 0 sequences so far.\n",
      "Please hold. This process can take up to 10 minutes to complete...\n",
      "<_io.StringIO object at 0x00000174AA12F550>\n",
      "Storing results as an XML file...\n",
      "Success! to_blast\\amynthas_seungpanensis_results.xml has been saved!\n",
      "Parsing XML file as dataframe...\n",
      "Saving dataframe as csv file...\n",
      "Success! Dataframe has successfully been saved as to_blast\\amynthas_seungpanensis_dataframe.csv. Press any key to exit function.\n",
      "Starting BLAST query on NCBI database...a_rosea.fna. Have BLASTed 0 sequences so far.\n",
      "Please hold. This process can take up to 10 minutes to complete...\n",
      "<_io.StringIO object at 0x00000174906950D0>\n",
      "Storing results as an XML file...\n",
      "Success! to_blast\\aporrectodea_rosea_results.xml has been saved!\n",
      "Parsing XML file as dataframe...\n",
      "Saving dataframe as csv file...\n",
      "Success! Dataframe has successfully been saved as to_blast\\aporrectodea_rosea_dataframe.csv. Press any key to exit function.\n",
      "Starting BLAST query on NCBI database...errestris.fna. Have BLASTed 0 sequences so far.\n",
      "Please hold. This process can take up to 10 minutes to complete...\n",
      "<_io.StringIO object at 0x00000174AA12F550>\n",
      "Storing results as an XML file...\n",
      "Success! to_blast\\lumbricus_terrestris_results.xml has been saved!\n",
      "Parsing XML file as dataframe...\n",
      "Saving dataframe as csv file...\n",
      "Success! Dataframe has successfully been saved as to_blast\\lumbricus_terrestris_dataframe.csv. Press any key to exit function.\n",
      "Starting BLAST query on NCBI database...birmanicus.fna. Have BLASTed 0 sequences so far.\n",
      "Please hold. This process can take up to 10 minutes to complete...\n",
      "<_io.StringIO object at 0x00000174AA12F550>\n",
      "Storing results as an XML file...\n",
      "Success! to_blast\\tonoscolex_birmanicus_results.xml has been saved!\n",
      "Parsing XML file as dataframe...\n",
      "Saving dataframe as csv file...\n",
      "Success! Dataframe has successfully been saved as to_blast\\tonoscolex_birmanicus_dataframe.csv. Press any key to exit function.\n",
      "Success! BlASTed a total of 0 sequences.\n"
     ]
    }
   ],
   "source": [
    "blast_multiple(folder_directory = \"to_blast\", blast_type=\"blastn\", max_hits= 5, megablast=False, e_value_threshold=1e-20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to work on a sequence downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(file_name: str, email_address: str, gene_name: str):\n",
    "    \"\"\"\"\n",
    "    A function that searches the genbank gene database and downloads multiple fasta sequence files\n",
    "    :inputs: file_name (including file extension) of a text file containing line separated species names to download sequences for,\n",
    "            email_address so NCBI knows who is requesting files data if anything goes awry,\n",
    "            gene_name containing the gene of interest to search for\n",
    "    :returns: None\n",
    "    \"\"\"\n",
    "\n",
    "    # Imports\n",
    "    from Bio import Entrez, SeqIO\n",
    "    import time\n",
    "\n",
    "    # Parameters\n",
    "    Entrez.api_key = \"0479a2cd7f6d9e97e14c35e4d49eba018509\"\n",
    "    Entrez.email = email_address\n",
    "\n",
    "    # Read text file\n",
    "    with open(file_name) as f:\n",
    "        species_names = [line.rstrip('\\n') for line in f]\n",
    "\n",
    "    # Start search\n",
    "    print(\"Starting search. Please hold as only 10 API requests can be made per second...\")\n",
    "    record_ids = []\n",
    "    record_counts = 0\n",
    "    for species_name in species_names:\n",
    "        handle = Entrez.esearch(db=\"nucleotide\", term=species_name + \"[Orgn] AND COI[Gene]\", idtype=\"acc\", retmax=10000)\n",
    "        record = Entrez.read(handle)\n",
    "        record_counts += int(record[\"Count\"])\n",
    "        record_ids += record[\"IdList\"]\n",
    "        print(\"Found \" + str(record_counts) + \" records so far...\", end=\"\\r\")\n",
    "        handle.close()\n",
    "\n",
    "        # Wait\n",
    "        time.sleep(0.15)\n",
    "    print(\"Search success! For a search of \" + str(len(species_names)) + \" species, a total of \" + str(record_counts) + \" genbank records containing COI genes were found!\")\n",
    "\n",
    "    # Start download\n",
    "    print(\"Starting sequence download process. Please hold as only 10 API requests can be made per second...\")\n",
    "    count = 0\n",
    "    for record_id in record_ids:\n",
    "        if not os.path.isfile(\"downloads//\" + gene_name + \"//\" + str(record_id) + \".fna\"):\n",
    "            net_handle = Entrez.efetch(db=\"nucleotide\", id=record_id, rettype=\"fasta\", retmode=\"text\", retmax=10000)\n",
    "            out_handle = open(\"downloads//\" + gene_name + \"//\" + str(record_id) + \".fna\", \"w\")\n",
    "            out_handle.write(net_handle.read())\n",
    "            out_handle.close()\n",
    "            net_handle.close()\n",
    "            count += 1\n",
    "            print(\"Downloading record \" + str(record_id) + \" Downloaded \" + str(count) + \" sequences so far...\", end=\"\\r\")\n",
    "\n",
    "            # Wait\n",
    "            time.sleep(0.15)\n",
    "\n",
    "    # End\n",
    "    print(\"Success! You can find your downloaded sequences in the downloads folder where this script is located!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the download function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting search. Please hold as only 10 API requests can be made per second...\n",
      "Search success! For a search of 82 species, a total of 2115 genbank records containing COI genes were found!\n",
      "Starting sequence download process. Please hold as only 10 API requests can be made per second...\n",
      "Success! You can find your downloaded sequences in the downloads folder where this script is located!\n"
     ]
    }
   ],
   "source": [
    "download(file_name=\"nematode_names.txt\", email_address=\"epay0001@student.monash.edu\", gene_name=\"COI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to write a function that checks for any duplicate sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(folder_directory: str, email_address: str):\n",
    "    \"\"\"\n",
    "    A function that scans a directory for any duplicate sequence files.\n",
    "    :inputs: folder_directory, containing the path of the folder\n",
    "    :returns: None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Imports\n",
    "    from Bio import SeqIO\n",
    "    from Bio import Entrez\n",
    "    Entrez.email = email_address\n",
    "    import time\n",
    "\n",
    "    # Loop through each file\n",
    "    file_paths_to_remove = set()\n",
    "\n",
    "    for filename_one in os.listdir(folder_directory):\n",
    "        f_one = os.path.join(folder_directory, filename_one)\n",
    "\n",
    "        # Check if the file exists\n",
    "        if os.path.isfile(f_one) and filename_one.split(\".\")[-1] == \"fna\":\n",
    "\n",
    "            # Check if file is already listed as a duplicate\n",
    "            if f_one not in file_paths_to_remove:\n",
    "            \n",
    "                with open(f_one) as handle_one:\n",
    "                    current_seq = next(SeqIO.parse(handle_one, \"fasta\"))\n",
    "                    # Check current_seq against every other file\n",
    "                    for filename_two in os.listdir(folder_directory):\n",
    "                        f_two = os.path.join(folder_directory, filename_two)\n",
    "                        \n",
    "                        # Check if the file exists\n",
    "                        if os.path.isfile(f_two) and filename_two.split(\".\")[-1] == \"fna\":\n",
    "\n",
    "                            # Check if file is already listed as a duplicate\n",
    "                            if f_two not in file_paths_to_remove:\n",
    "                            \n",
    "                                # Check if the file isn't the same as the one we are checking\n",
    "                                if filename_one != filename_two:\n",
    "                                    \n",
    "                                    with open(f_two) as handle_two:\n",
    "                                        check_seq = next(SeqIO.parse(handle_two, \"fasta\"))\n",
    "                                        \n",
    "                                        print(\"Comparing sequences between \" + filename_one + \" and \" + filename_two, end=\"\\r\")\n",
    "                                        # Check if the sequences of the fasta files are the same:                     \n",
    "                                        if current_seq.seq == check_seq.seq:\n",
    "                                            \n",
    "                                            # Checking current_seq origin\n",
    "                                            current_seq_id = current_seq.id.split(\".\")[0] + \".\" + current_seq.id.split(\".\")[1]\n",
    "                                            entrez_handle = Entrez.efetch(db=\"nucleotide\", id=current_seq_id, retmode=\"xml\", retmax=10000)\n",
    "                                            entrez_records = Entrez.parse(entrez_handle)\n",
    "                                            for entrez_record in entrez_records:\n",
    "                                                try:\n",
    "                                                    current_seq_origin = entrez_record['GBSeq_feature-table'][0]['GBFeature_quals'][6]['GBQualifier_value']\n",
    "                                                except IndexError:\n",
    "                                                    current_seq_origin = \"N/A\"\n",
    "                                            time.sleep(0.15)\n",
    "\n",
    "                                            # Checking check_seq origin\n",
    "                                            check_seq_id = check_seq.id.split(\".\")[0] + \".\" + check_seq.id.split(\".\")[1]\n",
    "                                            entrez_handle = Entrez.efetch(db=\"nucleotide\", id=check_seq_id, retmode=\"xml\", retmax=10000)\n",
    "                                            entrez_records = Entrez.parse(entrez_handle)\n",
    "                                            for entrez_record in entrez_records:\n",
    "                                                try:\n",
    "                                                    check_seq_origin = entrez_record['GBSeq_feature-table'][0]['GBFeature_quals'][6]['GBQualifier_value']\n",
    "                                                except IndexError:\n",
    "                                                    check_seq_origin = \"N/A\"\n",
    "                                            time.sleep(0.15)\n",
    "                                            \n",
    "                                            # Check if it's from the same country of origin\n",
    "                                            if current_seq_origin == check_seq_origin:\n",
    "                                                file_paths_to_remove.add(f_two)\n",
    "            \n",
    "\n",
    "    print(\"Found: \" + str(len(file_paths_to_remove)) + \" duplicate files to remove. Removing...\")  \n",
    "    \n",
    "    for file_name in file_paths_to_remove:\n",
    "        os.remove(file_name)\n",
    "        # Wait\n",
    "        time.sleep(0.15)\n",
    "    print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the check_duplicates function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24724/1067031721.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mremove_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_directory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"downloads\\COI\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memail_address\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"epay0001@student.monash.edu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24724/690670388.py\u001b[0m in \u001b[0;36mremove_duplicates\u001b[1;34m(folder_directory, email_address)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_one\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhandle_one\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                     \u001b[0mcurrent_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSeqIO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle_one\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fasta\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m                     \u001b[1;31m# Check current_seq against every other file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mfilename_two\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_directory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\Bio\\SeqIO\\Interfaces.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_close_stream\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "remove_duplicates(folder_directory = \"downloads\\COI\", email_address=\"epay0001@student.monash.edu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a combine sequences function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_seq(folder_directory: str):\n",
    "    \"\"\"\n",
    "    A function that combines all sequence files in a directory into a single sequence file.\n",
    "    :inputs: folder_directory, containing the path of the folder\n",
    "    :returns: None\n",
    "    \"\"\"\n",
    "    combined_file = open(os.path.join(folder_directory, \"!combined_sequences.fna\"), \"w\")\n",
    "    for f in os.listdir(folder_directory):\n",
    "        fh = open(os.path.join(folder_directory, f))\n",
    "        print(\"Currently scanning: \" + str(os.path.join(folder_directory, f)), end=\"\\r\")\n",
    "        for line in fh:\n",
    "            combined_file.write(line)\n",
    "        fh.close()\n",
    "    combined_file.close()\n",
    "    print(\"Successfully combined all sequences into a file called combined_sequences.fna in the \" + folder_directory + \" folder!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the combine_seq function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully combined all sequences into a file called combined_sequences.fna in the downloads\\COI folder!\n"
     ]
    }
   ],
   "source": [
    "combine_seq(\"downloads\\COI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to make a function that downloads all the metadata of a fasta file containing multiple sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_qualifier_index(arr, target):\n",
    "    \"\"\"\n",
    "\n",
    "    :inputs:\n",
    "    :returns:\n",
    "    \"\"\"\n",
    "\n",
    "    # Imports\n",
    "\n",
    "    for i in range(len(arr)):\n",
    "        if len(arr[i]) > 1:\n",
    "            for key, value in arr[i].items():\n",
    "                if value == target:\n",
    "                    return i\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_metadata(file_directory: str, email_address: str):\n",
    "    \"\"\"\n",
    "\n",
    "    :inputs:\n",
    "    :returns:\n",
    "    \"\"\"\n",
    "\n",
    "    # Imports\n",
    "    from Bio import SeqIO\n",
    "    from Bio import Entrez\n",
    "    Entrez.email = email_address\n",
    "    import time\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    with open(file_directory) as file_handle:\n",
    "        seq_record = SeqIO.parse(file_handle, \"fasta\")\n",
    "        # Create dataframe\n",
    "        df = pd.DataFrame()\n",
    "                \n",
    "        # Loop through records\n",
    "        for seq_record in SeqIO.parse(file_handle, \"fasta\"):\n",
    "                # Get record metadata\n",
    "                \n",
    "                entrez_handle = Entrez.efetch(db=\"nucleotide\", id=seq_record.id, retmode=\"xml\", retmax=10000)\n",
    "                entrez_records = Entrez.parse(entrez_handle)\n",
    "                \n",
    "                # Store results as XML\n",
    "                for entrez_record in entrez_records:\n",
    "                    print(\"Retrieving metdata for \" + str(seq_record.id) + \"...\", end=\"\\r\")\n",
    "                    # Authors\n",
    "                    try:\n",
    "                        authors = entrez_record['GBSeq_references'][0]['GBReference_authors']\n",
    "                    except:\n",
    "                        authors = np.NAN\n",
    "\n",
    "                    # Organism Name\n",
    "                    try:\n",
    "                        i = find_qualifier_index(entrez_record['GBSeq_feature-table'][0]['GBFeature_quals'], \"organism\")\n",
    "                        if i is not None:\n",
    "                            organism_name = entrez_record['GBSeq_feature-table'][0]['GBFeature_quals'][i]['GBQualifier_value']\n",
    "                        else:\n",
    "                            organism_name = np.NAN\n",
    "                    except:\n",
    "                        organism_name = np.NAN\n",
    "\n",
    "                    # Organelle\n",
    "                    try:\n",
    "                        i = find_qualifier_index(entrez_record['GBSeq_feature-table'][0]['GBFeature_quals'], \"organelle\")\n",
    "                        if i is not None:\n",
    "                            organelle = entrez_record['GBSeq_feature-table'][0]['GBFeature_quals'][i]['GBQualifier_value']\n",
    "                        else:\n",
    "                            organelle = np.NAN\n",
    "                    except:\n",
    "                        organelle = np.NAN\n",
    "\n",
    "                    # Isolation source\n",
    "                    try:\n",
    "                        i = find_qualifier_index(entrez_record['GBSeq_feature-table'][0]['GBFeature_quals'], \"isolation_source\")\n",
    "                        if i is not None:\n",
    "                            source = entrez_record['GBSeq_feature-table'][0]['GBFeature_quals'][i]['GBQualifier_value']\n",
    "                        else:\n",
    "                            source = np.NAN\n",
    "                    except:\n",
    "                        source = np.NAN\n",
    "\n",
    "                    # Taxon id\n",
    "                    try:\n",
    "                        i = find_qualifier_index(entrez_record['GBSeq_feature-table'][0]['GBFeature_quals'], \"db_xref\")\n",
    "                        if i is not None:\n",
    "                            taxon_id = entrez_record['GBSeq_feature-table'][0]['GBFeature_quals'][i]['GBQualifier_value'].split(\":\")[-1]\n",
    "                        else:\n",
    "                            taxon_id = np.NAN\n",
    "                    except:\n",
    "                        taxon_id = v\n",
    "\n",
    "                    # Country of origin\n",
    "                    try:\n",
    "                        i = find_qualifier_index(entrez_record['GBSeq_feature-table'][0]['GBFeature_quals'], \"country\")\n",
    "                        if i is not None:\n",
    "                            country = entrez_record['GBSeq_feature-table'][0]['GBFeature_quals'][i]['GBQualifier_value']\n",
    "                        else:\n",
    "                            country = np.NAN\n",
    "                    except:\n",
    "                        country = np.NAN\n",
    "\n",
    "                    # Lat/lon\n",
    "                    try:\n",
    "                        i = find_qualifier_index(entrez_record['GBSeq_feature-table'][0]['GBFeature_quals'], \"lat_lon\")\n",
    "                        if i is not None:\n",
    "                            lat_lon = entrez_record['GBSeq_feature-table'][0]['GBFeature_quals'][i]['GBQualifier_value']\n",
    "                        else:\n",
    "                            lat_lon = np.NAN\n",
    "                    except:\n",
    "                        lat_lon = np.NAN\n",
    "\n",
    "                    # Gene\n",
    "                    try:\n",
    "                        i = find_qualifier_index(entrez_record['GBSeq_feature-table'][2]['GBFeature_quals'], \"gene\")\n",
    "                        if i is not None:\n",
    "                            gene = entrez_record['GBSeq_feature-table'][1]['GBFeature_quals'][i]['GBQualifier_value']\n",
    "                        else:\n",
    "                            gene = np.NAN\n",
    "                    except:\n",
    "                        gene = np.NAN\n",
    "\n",
    "                    # Codon start\n",
    "                    try:\n",
    "                        i = find_qualifier_index(entrez_record['GBSeq_feature-table'][2]['GBFeature_quals'], \"codon_start\")\n",
    "                        if i is not None:\n",
    "                            codon_start = entrez_record['GBSeq_feature-table'][2]['GBFeature_quals'][i]['GBQualifier_value']\n",
    "                        else:\n",
    "                            codon_start = np.NAN\n",
    "                    except:\n",
    "                        codon_start = np.NAN\n",
    "\n",
    "                    # Translation table\n",
    "                    try:\n",
    "                        i = find_qualifier_index(entrez_record['GBSeq_feature-table'][2]['GBFeature_quals'], \"transl_table\")\n",
    "                        if i is not None:\n",
    "                            transl_table = entrez_record['GBSeq_feature-table'][2]['GBFeature_quals'][i]['GBQualifier_value']\n",
    "                        else:\n",
    "                            transl_table = np.NAN\n",
    "                    except:\n",
    "                        transl_table = np.NAN\n",
    "\n",
    "                    # Translation\n",
    "                    try:\n",
    "                        i = find_qualifier_index(entrez_record['GBSeq_feature-table'][2]['GBFeature_quals'], \"translation\")\n",
    "                        if i is not None:\n",
    "                            translation = entrez_record['GBSeq_feature-table'][2]['GBFeature_quals'][i]['GBQualifier_value']\n",
    "                        else:\n",
    "                            translation = np.NAN\n",
    "                    except:\n",
    "                        translation = np.NAN\n",
    "                \n",
    "                # Create row\n",
    "                row = {'id' : seq_record.id, 'organism' : organism_name, 'authors' : authors, 'organelle' : organelle, 'isolation_source' : source, \n",
    "                        'taxon_id' : taxon_id, 'country' : country, 'lat_lon' : lat_lon, 'gene' : gene, 'codon_start' : codon_start, 'sequence' : seq_record.seq, \n",
    "                        'transl_table' : transl_table, 'translation' : translation}\n",
    "                df = df.append(row, ignore_index=True)\n",
    "\n",
    "                time.sleep(0.15)\n",
    "        \n",
    "        save_path_list = file_directory.split(\"\\\\\")\n",
    "        save_path = \"\"\n",
    "\n",
    "        for i in range(len(save_path_list)):\n",
    "            if i != len(save_path_list) - 1:\n",
    "                save_path += save_path_list[i]\n",
    "                save_path += \"\\\\\"\n",
    "\n",
    "        df.to_csv(save_path + \"!record_metadata.csv\")\n",
    "        print(\"Successfully saved metadata as: !record_metadata.csv in the: \" + save_path + \" directory!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved metadata as: !record_metadata.csv in the: downloads\\COI\\ directory!\n"
     ]
    }
   ],
   "source": [
    "get_metadata(file_directory=\"downloads\\COI\\!combined_sequences.fna\", email_address=\"epay0001@student.monash.edu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I've added more Nematodes to the nematode_names.txt file, I want to check if there are any duplicate species:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nematodes(file_name: str):\n",
    "    unique_names = set()\n",
    "    dupelicate_names = []\n",
    "    with open(file_name) as f:\n",
    "        species_names = [line.rstrip('\\n') for line in f]\n",
    "        for species_name in species_names:\n",
    "            if species_name not in unique_names:\n",
    "                unique_names.add(species_name)\n",
    "            else:\n",
    "                dupelicate_names.append(species_name)\n",
    "    return dupelicate_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Heterodera trifolii', 'Radopholus nativus']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_nematodes(file_name=\"nematode_names.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can now manually remove the above species from the text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run all of the functions with this new list of nematodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting search. Please hold as only 10 API requests can be made per second...\n",
      "Search success! For a search of 304 species, a total of 2556 genbank records containing COI genes were found!\n",
      "Starting sequence download process. Please hold as only 10 API requests can be made per second...\n",
      "Success! You can find your downloaded sequences in the downloads folder where this script is located!\n"
     ]
    }
   ],
   "source": [
    "download(file_name=\"nematode_names.txt\", email_address=\"epay0001@student.monash.edu\", gene_name=\"COI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: 1136 duplicate files to remove. Removing...\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "check_duplicates(folder_directory = \"downloads\\COI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully combined all sequences into a file called combined_sequences.fna in the downloads\\COI folder!\n"
     ]
    }
   ],
   "source": [
    "combine_seq(\"downloads\\COI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved metadata as: !record_metadata.csv in the: downloads\\COI\\ directory!\n"
     ]
    }
   ],
   "source": [
    "get_metadata(\"downloads\\COI\\!combined_sequences.fna\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f08154012ddadd8e950e6e9e035c7a7b32c136e7647e9b7c77e02eb723a8bedb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
